%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for 김당찬 at 2023-11-24 11:46:38 +0900 

%% Saved with string encoding Unicode (UTF-8)
@book{pearl_2009,
	title        = {Causality},
	author       = {Pearl, Judea},
	year         = 2009,
	publisher    = {Cambridge University Press},
	doi          = {10.1017/CBO9780511803161},
	place        = {Cambridge},
	edition      = 2
}
@inproceedings{Shipster2006semimarkovian,
	title        = {Identification of Joint Interventional Distributions in Recursive Semi-Markovian Causal Models},
	author       = {Shpitser, Ilya and Pearl, Judea},
	year         = 2006,
	booktitle    = {Proceedings of the 21st National Conference on Artificial Intelligence - Volume 2},
	location     = {Boston, Massachusetts},
	publisher    = {AAAI Press},
	series       = {AAAI'06},
	pages        = {1219–1226},
	isbn         = 9781577352815,
	abstract     = {This paper is concerned with estimating the effects of actions from causal assumptions, represented concisely as a directed graph, and statistical knowledge, given as a probability distribution. We provide a necessary and sufficient graphical condition for the cases when the causal effect of an arbitrary set of variables on another arbitrary set can be determined uniquely from the available information, as well as an algorithm which computes the effect whenever this condition holds. Furthermore, we use our results to prove completeness of do-calculus [Pearl, 1995], and a version of an identification algorithm in [Tian, 2002] for the same identification problem. Finally, we derive a complete characterization of semi-Markovian models in which all causal effects are identifiable.},
	numpages     = 8
}
@inproceedings{rezende15normalizingflows,
	title        = {Variational Inference with Normalizing Flows},
	author       = {Rezende, Danilo and Mohamed, Shakir},
	year         = 2015,
	month        = {07--09 Jul},
	booktitle    = {Proceedings of the 32nd International Conference on Machine Learning},
	publisher    = {PMLR},
	address      = {Lille, France},
	series       = {Proceedings of Machine Learning Research},
	volume       = 37,
	pages        = {1530--1538},
	url          = {https://proceedings.mlr.press/v37/rezende15.html},
	abstract     = {The choice of the approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.},
	editor       = {Bach, Francis and Blei, David},
	pdf          = {http://proceedings.mlr.press/v37/rezende15.pdf},
	bdsk-url-1   = {https://proceedings.mlr.press/v37/rezende15.html}
}
@misc{tomczak2017ccliniaf,
	title        = {Improving Variational Auto-Encoders using convex combination linear Inverse Autoregressive Flow},
	author       = {Jakub M. Tomczak and Max Welling},
	year         = 2017,
	archiveprefix = {arXiv},
	date-modified = {2023-11-24 11:44:47 +0900},
	eprint       = {1706.02326},
	primaryclass = {stat.ML}
}
@article{wang2017empirical,
	title        = {Empirical likelihood for linear structural equation models with dependent errors},
	author       = {Wang, Y. Samuel and Drton, Mathias},
	year         = 2017,
	journal      = {Stat},
	volume       = 6,
	number       = 1,
	pages        = {434--447},
	doi          = {https://doi.org/10.1002/sta4.169},
	url          = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sta4.169},
	abstract     = {We consider linear structural equation models that are associated with mixed graphs. The structural equations in these models only involve observed variables, but their idiosyncratic error terms are allowed to be correlated and non-Gaussian. We propose empirical likelihood procedures for inference and suggest several modifications, including a profile likelihood, in order to improve tractability and performance of the resulting methods. Through simulations, we show that when the error distributions are non-Gaussian, the use of empirical likelihood and the proposed modifications may increase statistical efficiency and improve assessment of significance. Copyright {\copyright} 2017 John Wiley \& Sons, Ltd.},
	date-modified = {2023-11-24 11:45:41 +0900},
	eprint       = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/sta4.169},
	keywords     = {causal inference, empirical likelihood, graphical model, structural equation model},
	bdsk-url-1   = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sta4.169},
	bdsk-url-2   = {https://doi.org/10.1002/sta4.169}
}
@inproceedings{kingma2016iaf,
	title        = {Improved Variational Inference with Inverse Autoregressive Flow},
	author       = {Kingma, Durk P and Salimans, Tim and Jozefowicz, Rafal and Chen, Xi and Sutskever, Ilya and Welling, Max},
	year         = 2016,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 29,
	url          = {https://proceedings.neurips.cc/paper_files/paper/2016/file/ddeebdeefdb7e7e7a697e1c3e3d8ef54-Paper.pdf},
	date-modified = {2023-11-24 11:45:15 +0900},
	editor       = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
	bdsk-url-1   = {https://proceedings.neurips.cc/paper_files/paper/2016/file/ddeebdeefdb7e7e7a697e1c3e3d8ef54-Paper.pdf}
}
@misc{tomczak2017householder,
	title        = {Improving Variational Auto-Encoders using Householder Flow},
	author       = {Jakub M. Tomczak and Max Welling},
	year         = 2017,
	archiveprefix = {arXiv},
	date-modified = {2023-11-24 11:44:38 +0900},
	eprint       = {1611.09630},
	primaryclass = {cs.LG}
}
@misc{zhang2019dvae,
	title        = {D-VAE: A Variational Autoencoder for Directed Acyclic Graphs},
	author       = {Muhan Zhang and Shali Jiang and Zhicheng Cui and Roman Garnett and Yixin Chen},
	year         = 2019,
	archiveprefix = {arXiv},
	eprint       = {1904.11088},
	primaryclass = {cs.LG}
}
@book{pml2Book,
	title        = {Probabilistic Machine Learning: Advanced Topics},
	author       = {Kevin P. Murphy},
	year         = 2023,
	publisher    = {MIT Press},
	url          = {http://probml.github.io/book2},
	bdsk-url-1   = {http://probml.github.io/book2}
}
@misc{rolinek2019variationalpca,
	title        = {Variational Autoencoders Pursue PCA Directions (by Accident)},
	author       = {Michal Rolinek and Dominik Zietlow and Georg Martius},
	year         = 2019,
	archiveprefix = {arXiv},
	date-modified = {2023-11-24 11:46:20 +0900},
	eprint       = {1812.06775},
	primaryclass = {cs.LG}
}
@misc{frazier2018tutorial,
	title        = {A Tutorial on Bayesian Optimization},
	author       = {Peter I. Frazier},
	year         = 2018,
	archiveprefix = {arXiv},
	eprint       = {1807.02811},
	primaryclass = {stat.ML}
}
@inproceedings{Yang_2021_CVPR,
	title        = {CausalVAE: Disentangled Representation Learning via Neural Structural Causal Models},
	author       = {Yang, Mengyue and Liu, Furui and Chen, Zhitang and Shen, Xinwei and Hao, Jianye and Wang, Jun},
	year         = 2021,
	month        = {June},
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	pages        = {9593--9602}
}
@misc{kipf2016variational,
	title        = {Variational Graph Auto-Encoders},
	author       = {Thomas N. Kipf and Max Welling},
	year         = 2016,
	archiveprefix = {arXiv},
	eprint       = {1611.07308},
	primaryclass = {stat.ML}
}
@misc{burgess2018beta,
	title        = {Understanding disentangling in $\beta$-VAE},
	author       = {Christopher P. Burgess and Irina Higgins and Arka Pal and Loic Matthey and Nick Watters and Guillaume Desjardins and Alexander Lerchner},
	year         = 2018,
	archiveprefix = {arXiv},
	date-modified = {2023-11-24 11:45:57 +0900},
	eprint       = {1804.03599},
	primaryclass = {stat.ML}
}
@article{mlpsvd,
	title        = {Auto-association by multilayer perceptrons and singular value decomposition},
	author       = {Bourlard, H. and Kamp, Y.},
	year         = 1988,
	journal      = {Biological Cybernetics},
	volume       = 59,
	number       = 4,
	pages        = {291--294},
	doi          = {10.1007/BF00332918},
	isbn         = {1432-0770},
	url          = {https://doi.org/10.1007/BF00332918},
	abstract     = {The multilayer perceptron, when working in auto-association mode, is sometimes considered as an interesting candidate to perform data compression or dimensionality reduction of the feature space in information processing applications. The present paper shows that, for auto-association, the nonlinearities of the hidden units are useless and that the optimal parameter values can be derived directly by purely linear techniques relying on singular value decomposition and low rank matrix approximation, similar in spirit to the well-known Karhunen-Lo{\`e}ve transform. This approach appears thus as an efficient alternative to the general error back-propagation algorithm commonly used for training multilayer perceptrons. Moreover, it also gives a clear interpretation of the r{\^o}le of the different parameters.},
	date         = {1988/09/01},
	date-added   = {2023-10-18 21:02:45 +0900},
	date-modified = {2023-10-18 21:02:45 +0900},
	id           = {Bourlard1988},
	bdsk-url-1   = {https://doi.org/10.1007/BF00332918}
}
@misc{li2020dirichlet,
	title        = {Dirichlet Graph Variational Autoencoder},
	author       = {Jia Li and Tomasyu Yu and Jiajin Li and Honglei Zhang and Kangfei Zhao and YU Rong and Hong Cheng and Junzhou Huang},
	year         = 2020,
	archiveprefix = {arXiv},
	eprint       = {2010.04408},
	primaryclass = {cs.LG}
}
@article{ChickeringGES,
	title        = {Optimal Structure Identification with Greedy Search},
	author       = {Chickering, David Maxwell},
	year         = 2003,
	month        = {mar},
	journal      = {J. Mach. Learn. Res.},
	publisher    = {JMLR.org},
	volume       = 3,
	number       = {null},
	pages        = {507--554},
	doi          = {10.1162/153244303321897717},
	issn         = {1532-4435},
	url          = {https://doi.org/10.1162/153244303321897717},
	abstract     = {In this paper we prove the so-called "Meek Conjecture". In particular, we show that if a DAG H is an independence map of another DAG G, then there exists a finite sequence of edge additions and covered edge reversals in G such that (1) after each edge modification H remains an independence map of G and (2) after all modifications G =H. As shown by Meek (1997), this result has an important consequence for Bayesian approaches to learning Bayesian networks from data: in the limit of large sample size, there exists a two-phase greedy search algorithm that---when applied to a particular sparsely-connected search space---provably identifies a perfect map of the generative distribution if that perfect map is a DAG. We provide a new implementation of the search space, using equivalence classes as states, for which all operators used in the greedy search can be scored efficiently using local functions of the nodes in the domain. Finally, using both synthetic and real-world datasets, we demonstrate that the two-phase greedy approach leads to good solutions when learning with finite sample sizes.},
	issue_date   = {3/1/2003},
	numpages     = 48,
	bdsk-url-1   = {https://doi.org/10.1162/153244303321897717}
}
@article{GhoshalSEM,
	title        = {Learning linear structural equation models in polynomial time and sample complexity},
	author       = {Asish Ghoshal and Jean Honorio},
	year         = 2017,
	journal      = {CoRR},
	volume       = {abs/1707.04673},
	url          = {http://arxiv.org/abs/1707.04673},
	bibsource    = {dblp computer science bibliography, https://dblp.org},
	biburl       = {https://dblp.org/rec/journals/corr/GhoshalH17aa.bib},
	eprint       = {1707.04673},
	eprinttype   = {arXiv},
	timestamp    = {Mon, 13 Aug 2018 16:48:07 +0200},
	bdsk-url-1   = {http://arxiv.org/abs/1707.04673}
}
@article{ChickeringNPhard,
	title        = {Large-Sample Learning of Bayesian Networks is NP-Hard},
	author       = {David Maxwell Chickering and Christopher Meek and David Heckerman},
	year         = 2012,
	journal      = {CoRR},
	volume       = {abs/1212.2468},
	url          = {http://arxiv.org/abs/1212.2468},
	bibsource    = {dblp computer science bibliography, https://dblp.org},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1212-2468.bib},
	eprint       = {1212.2468},
	eprinttype   = {arXiv},
	timestamp    = {Mon, 13 Aug 2018 16:47:15 +0200},
	bdsk-url-1   = {http://arxiv.org/abs/1212.2468}
}
@misc{zheng2018dags,
	title        = {DAGs with NO TEARS: Continuous Optimization for Structure Learning},
	author       = {Xun Zheng and Bryon Aragam and Pradeep Ravikumar and Eric P. Xing},
	year         = 2018,
	archiveprefix = {arXiv},
	eprint       = {1803.01422},
	primaryclass = {stat.ML}
}
@article{yu2019daggnn,
	title        = {DAG-GNN: DAG Structure Learning with Graph Neural Networks},
	author       = {Yue Yu and Jie Chen and Tian Gao and Mo Yu},
	year         = 2019,
	archiveprefix = {arXiv},
	eprint       = {1904.10098},
	primaryclass = {cs.LG}
}
@book{Spirtes2000,
	title        = {Causation, Prediction, and Search},
	author       = {Spirtes, P. and Glymour, C. and Scheines, R.},
	year         = 2000,
	publisher    = {MIT press},
	added-at     = {2009-09-12T19:19:34.000+0200},
	biburl       = {https://www.bibsonomy.org/bibtex/2e2b107e8fd3469c8b0e944ca37a559f3/mozaher},
	edition      = {2nd},
	interhash    = {559e17fcd12a76214629ba6c4efe3f9a},
	intrahash    = {e2b107e8fd3469c8b0e944ca37a559f3},
	keywords     = {imported},
	owner        = {Mozaherul Hoque},
	review       = {PC algorithm},
	timestamp    = {2009-09-12T19:19:43.000+0200}
}
@article{kingma2013auto,
	title        = {Auto-encoding variational bayes},
	author       = {Kingma, Diederik P and Welling, Max},
	year         = 2013,
	journal      = {arXiv preprint arXiv:1312.6114}
}
@misc{zhang2018sentencestate,
	title        = {Sentence-State LSTM for Text Representation},
	author       = {Yue Zhang and Qi Liu and Linfeng Song},
	year         = 2018,
	archiveprefix = {arXiv},
	eprint       = {1805.02474},
	primaryclass = {cs.CL}
}
@misc{zhang2020document,
	title        = {Every Document Owns Its Structure: Inductive Text Classification via Graph Neural Networks},
	author       = {Yufeng Zhang and Xueli Yu and Zeyu Cui and Shu Wu and Zhongzhen Wen and Liang Wang},
	year         = 2020,
	archiveprefix = {arXiv},
	eprint       = {2004.13826},
	primaryclass = {cs.CL}
}
@misc{marcheggiani2017encoding,
	title        = {Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling},
	author       = {Diego Marcheggiani and Ivan Titov},
	year         = 2017,
	archiveprefix = {arXiv},
	eprint       = {1703.04826},
	primaryclass = {cs.CL}
}
@misc{beck2018graphtosequence,
	title        = {Graph-to-Sequence Learning using Gated Graph Neural Networks},
	author       = {Daniel Beck and Gholamreza Haffari and Trevor Cohn},
	year         = 2018,
	archiveprefix = {arXiv},
	eprint       = {1806.09835},
	primaryclass = {cs.CL}
}
@misc{schlichtkrull2017modeling,
	title        = {Modeling Relational Data with Graph Convolutional Networks},
	author       = {Michael Schlichtkrull and Thomas N. Kipf and Peter Bloem and Rianne van den Berg and Ivan Titov and Max Welling},
	year         = 2017,
	archiveprefix = {arXiv},
	eprint       = {1703.06103},
	primaryclass = {stat.ML}
}
@misc{peng2017crosssentence,
	title        = {Cross-Sentence N-ary Relation Extraction with Graph LSTMs},
	author       = {Nanyun Peng and Hoifung Poon and Chris Quirk and Kristina Toutanova and Wen-tau Yih},
	year         = 2017,
	archiveprefix = {arXiv},
	eprint       = {1708.03743},
	primaryclass = {cs.CL}
}
@inproceedings{zhao2020vaeforsparse,
	title        = {Variational Autoencoders for Sparse and Overdispersed Discrete Data},
	author       = {Zhao, He and Rai, Piyush and Du, Lan and Buntine, Wray and Phung, Dinh and Zhou, Mingyuan},
	year         = 2020,
	month        = {26--28 Aug},
	booktitle    = {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = 108,
	pages        = {1684--1694},
	url          = {https://proceedings.mlr.press/v108/zhao20c.html},
	abstract     = {Many applications, such as text modelling, high-throughput sequencing, and recommender systems, require analysing sparse, high-dimensional, and overdispersed discrete (count or binary) data. Recent deep probabilistic models based on variational autoencoders (VAE) have shown promising results on discrete data but may have inferior modelling performance due to the insufficient capability in modelling overdispersion and model misspecification. To address these issues, we develop a VAE-based framework using the negative binomial distribution as the data distribution. We also provide an analysis of its properties vis-{\`a}-vis other models. We conduct extensive experiments on three problems from discrete data analysis: text analysis/topic modelling, collaborative filtering, and multi-label learning. Our models outperform state-of-the-art approaches on these problems, while also capturing the phenomenon of overdispersion more effectively.},
	date-modified = {2023-11-24 11:46:15 +0900},
	editor       = {Chiappa, Silvia and Calandra, Roberto},
	pdf          = {http://proceedings.mlr.press/v108/zhao20c/zhao20c.pdf},
	bdsk-url-1   = {https://proceedings.mlr.press/v108/zhao20c.html}
}
@article{epvae,
	title        = {EP Structured Variational Autoencoders},
	author       = {So, Jonathan and Townsend, James and Gaujac, Benoit},
	year         = 2018,
	journal      = {1st Symposium on Advances in Approximate Bayesian Inference}
}
@article{wuetal2011,
	title        = {Enhancing text representation for classification tasks with semantic graph structures},
	author       = {Wu, Jiangning and Xuan, Zhaoguo and Pan, Donghua},
	year         = 2011,
	month        = {05},
	journal      = {International Journal of Innovative Computing, Information and Control},
	volume       = 7
}
@article{CLOUGH2016235,
	title        = {What is the dimension of citation space?},
	author       = {James R. Clough and Tim S. Evans},
	year         = 2016,
	journal      = {Physica A: Statistical Mechanics and its Applications},
	volume       = 448,
	pages        = {235--247},
	doi          = {https://doi.org/10.1016/j.physa.2015.12.053},
	issn         = {0378-4371},
	url          = {https://www.sciencedirect.com/science/article/pii/S037843711501081X},
	abstract     = {Citation networks represent the flow of information between agents. They are constrained in time and so form directed acyclic graphs which have a causal structure. Here we provide novel quantitative methods to characterise that structure by adapting methods used in the causal set approach to quantum gravity by considering the networks to be embedded in a Minkowski spacetime and measuring its dimension using Myrheim--Meyer and Midpoint-scaling estimates. We illustrate these methods on citation networks from the arXiv, supreme court judgements from the USA, and patents and find that otherwise similar citation networks have measurably different dimensions. We suggest that these differences can be interpreted in terms of the level of diversity or narrowness in citation behaviour.},
	keywords     = {Citation network, Directed acyclic graph, Minkowski space, Dimension, Causal set, Network geometry},
	bdsk-url-1   = {https://www.sciencedirect.com/science/article/pii/S037843711501081X},
	bdsk-url-2   = {https://doi.org/10.1016/j.physa.2015.12.053}
}
@article{erdds1959random,
	title        = {On random graphs I},
	author       = {ERDdS, P and R\&wi, A},
	year         = 1959,
	journal      = {Publ. math. debrecen},
	volume       = 6,
	number       = {290-297},
	pages        = 18
}
@misc{reparametrization,
	title        = {Implicit Reparameterization Gradients},
	author       = {Michael Figurnov and Shakir Mohamed and Andriy Mnih},
	year         = 2019,
	archiveprefix = {arXiv},
	eprint       = {1805.08498},
	primaryclass = {cs.LG}
}

@article{Bongers_2021,
	title        = {Foundations of structural causal models with cycles and latent variables},
	author       = {Bongers, Stephan and Forré, Patrick and Peters, Jonas and Mooij, Joris M.},
	year         = 2021,
	month        = oct,
	journal      = {The Annals of Statistics},
	publisher    = {Institute of Mathematical Statistics},
	volume       = 49,
	number       = 5,
	doi          = {10.1214/21-aos2064},
	issn         = {0090-5364},
	url          = {http://dx.doi.org/10.1214/21-AOS2064}
}
@book{peters,
	title        = {Elements of Causal Inference: Foundations and Learning Algorithms},
	author       = {Peters, Jonas and Janzing, Dominik and Schlkopf, Bernhard},
	year         = 2017,
	publisher    = {The MIT Press},
	isbn         = {0262037319},
	abstract     = {A concise and self-contained introduction to causal inference, increasingly important in data science and machine learning. The mathematization of causality is a relatively recent development, and has become increasingly important in data science and machine learning. This book offers a self-contained and concise introduction to causal models and how to learn them from data. After explaining the need for causal models and discussing some of the principles underlying causal inference, the book teaches readers how to use causal models: how to compute intervention distributions, how to infer causal models from observational and interventional data, and how causal ideas could be exploited for classical machine learning problems. All of these topics are discussed first in terms of two variables and then in the more general multivariate case. The bivariate case turns out to be a particularly hard problem for causal learning because there are no conditional independences as used by classical methods for solving multivariate cases. The authors consider analyzing statistical asymmetries between cause and effect to be highly instructive, and they report on their decade of intensive research into this problem. The book is accessible to readers with a background in machine learning or statistics, and can be used in graduate courses or as a reference for researchers. The text includes code snippets that can be copied and pasted, exercises, and an appendix with a summary of the most important technical concepts.}
}
@book{Goodfellow,
	title        = {Deep Learning},
	author       = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year         = 2016,
	publisher    = {MIT Press},
	url          = {http://www.deeplearningbook.org},
	note         = {Book in preparation for MIT Press},
	added-at     = {2017-03-13T20:27:27.000+0100},
	biburl       = {https://www.bibsonomy.org/bibtex/2175f81afff897a68829e4d30c080a8fb/hotho},
	interhash    = {62814dec510d5c55b0b38ad85a6c748d},
	intrahash    = {175f81afff897a68829e4d30c080a8fb},
	keywords     = {book deep learning toread},
	timestamp    = {2017-04-14T13:44:20.000+0200}
}
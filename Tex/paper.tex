\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{amssymb}
\usepackage[version=4]{mhchem}
\usepackage{stmaryrd}
\usepackage{bbold}
\usepackage{enumitem}
\usepackage{hanging} %reference indent
\usepackage[a4paper,left=25mm,right=25mm,top=40mm,bottom=40mm]{geometry}
\usepackage{sectsty}
\sectionfont{\fontsize{10}{15}\selectfont}
\subsectionfont{\fontsize{10}{15}}
\usepackage[mathlines,switch]{lineno}
\usepackage{amsthm}
\usepackage{kotex}
\usepackage{xcolor}
\usepackage{graphicx} % insert image
\usepackage{multicol} % multi column
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{float}
\usepackage{tabularx}
\usepackage{multirow} % multirow in table
\usepackage{tikz}
\usepackage{subcaption}
\usepackage{hyperref}
\modulolinenumbers[5]
% \linenumbers

\usepackage{titlesec}
\titleformat{\section}
{\normalfont\fontsize{12}{12}\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}
{\normalfont\fontsize{11}{11}\bfseries}{\thesubsection}{1em}{}
\titlespacing{\paragraph}{0pt}{\parskip}{5pt}
\titleformat{\title}
{\normalfont\fontsize{13}{13}\bfseries}{\thetitle}{1em}{}

\usepackage{indentfirst} % first line indent
\setlength{\parindent}{10pt}

\renewcommand{\arraystretch}{1.1}

\title{
\textbf{Learning Semi-Markovian DAGs with Flow based VAE}}

\author{Dangchan Kim, Byungguk Kang, Jaeseok Kim, Minchan Kim}

\begin{document}
\maketitle

\begin{abstract}
    We propose a method to learn the structure of a semi-Markovian directed acyclic graph, i.e. mixed graph with directed and undirected edges using a flow-based variational autoencoder. The proposed method is based on the assumption that the noise matrix in the linear structural equation model is a latent variable. In order to learn the structure of the mixed graph, we used an inverse autoregressive flow to approximate the dependency structure of the noise matrix and their prior distribution. We conducted experiments on simulated data adjusting the number of nodes and the proportion of bidirectional edges. The code is available at \url{https://github.io/ddangchani/NFG-VAE}.
\end{abstract}

\section{Introduction}

Here is the introduction.

\section{Directed Acyclic Graphs and Mixed Graphs}
\subsection{Directed Acyclic Graphs}

Here is the introduction of DAGs

\subsection{Mixed Graphs}

Here is the introduction of mixed graphs

\subsection{Graph Learning}

Here is the introduction of graph learning

\section{Variational Autoencoders and Normalizing Flows}

\subsection{Variational Autoencoders}

Here is the introduction of VAEs

\subsection{Normalizing Flows}

Normalizing flows are a method of transforming a simple distribution into a complex distribution. \cite{rezende15normalizingflows} Let $z$ be a random variable with a simple distribution $p(z)$, and $x$ be a random variable with a complex distribution $p(x)$. Then, we can transform $z$ into $x$ by a series of invertible transformations $f_k$ as follows:
\begin{equation}
    x = f_K \circ f_{K-1} \circ \cdots \circ f_1(z)
\end{equation}

The probability density function of $x$ can be obtained by the change of variables formula:
\begin{equation}
    p(x) = p(z) \left| \det \left( \frac{\partial f_K \circ f_{K-1} \circ \cdots \circ f_1(z)}{\partial z} \right) \right|
\end{equation}

There are several types of normalizing flows, such as Householder flow \cite{tomczak2017householder}, planar flow \cite{rezende15normalizingflows}, etc. In this paper, we use the inverse autoregressive flow (IAF) \cite{kingma2016iaf}, especially linear IAF. The linear IAF is defined as follows:
\begin{equation}
    \bf z_t = \bf \mu_t + \bf \sigma_t \odot \bf z_{t-1}.
\end{equation}
Linear IAF is the simplest case of IAF, which transforms a multivariate Gaussian with diagonal covariance to a multivariate Gaussian with full covariance. The transformation is invertible and the determinant of the Jacobian is easily computable. To use linear IAF at VAE, we need to produce an extra output $\mathbf{L(x)}$ from the encoder network. The full-covariance Gaussian distribution is obtained by the following transformation:
\begin{equation}
    \mathbf{z}_T = \mathbf{L(x)} \cdot \mathbf{z}_0.
\end{equation}


\section{Method}

\subsection{Model}

Here is the introduction of our model

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{fig/model.png}
    \caption{Architecture of the proposed model.}
    \label{diagram}
\end{figure}

\subsection{Learning}

Here is the introduction (acyclicity constraint, etc.)

\section{Experiment}

In this section, we conduct experiments on simulated data to evaluate the performance of our proposed method. We compare our method with the DAG-GNN \cite{yu2019daggnn} and NOTEARS \cite{zheng2018dags} with random graph datasets. We used a thresholding value of extracting graph as 0.3 which is the same as DAG-GNN and NOTEARS.

\paragraph*{Datasets}



\paragraph*{Evaluation}

Here is the introduction of our evaluation(SHD, etc.)

\subsection{Independent Noise Case}

Here is the introduction of our experiment on independent noise case

\subsection{Dependent Noise Case}

Here is the introduction of our experiment on dependent noise case

\section{Conclusion}

Here is the conclusion

% Biblography
\bibliographystyle{ieeetr}
\bibliography{references}

\end{document}


\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{amssymb}
\usepackage[version=4]{mhchem}
\usepackage{stmaryrd}
\usepackage{bbold}
\usepackage{enumitem}
\usepackage{hanging} %reference indent
\usepackage[a4paper,left=25mm,right=25mm,top=40mm,bottom=40mm]{geometry}
\usepackage{sectsty}
\sectionfont{\fontsize{10}{15}\selectfont}
\subsectionfont{\fontsize{10}{15}}
\usepackage[mathlines,switch]{lineno}
\usepackage{amsthm}
\usepackage{kotex}
\usepackage{xcolor}
\usepackage{graphicx} % insert image
\usepackage{multicol} % multi column
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{float}
\usepackage{tabularx}
\usepackage{multirow} % multirow in table
\usepackage{tikz}
\usepackage{subcaption}
\usepackage{hyperref}
\modulolinenumbers[5]
% \linenumbers

\usepackage{titlesec}
\titleformat{\section}
{\normalfont\fontsize{12}{12}\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}
{\normalfont\fontsize{11}{11}\bfseries}{\thesubsection}{1em}{}
\titlespacing{\paragraph}{0pt}{\parskip}{5pt}
\titleformat{\title}
{\normalfont\fontsize{13}{13}\bfseries}{\thetitle}{1em}{}

\usepackage{indentfirst} % first line indent
\setlength{\parindent}{10pt}

\renewcommand{\arraystretch}{1.1}

\title{
\textbf{Learning Semi-Markovian DAGs with Flow based VAE}}

\author{\normalsize\bfseries Dangchan Kim, Byungguk Kang, Jaeseok Kim, Minchan Kim}

\begin{document}
\maketitle

\begin{abstract}
    We propose a method to learn the structure of a semi-Markovian directed acyclic graph, i.e. mixed graph with directed and undirected edges using a flow-based variational autoencoder. The proposed method is based on the assumption that the noise matrix in the linear structural equation model is a latent variable. In order to learn the structure of the mixed graph, we used an inverse autoregressive flow to approximate the dependency structure of the noise matrix and their prior distribution. We conducted experiments on simulated data adjusting the number of nodes and the proportion of bidirectional edges. The code is available at \url{https://github.io/ddangchani/NFG-VAE}.
\end{abstract}

\section{Introduction}

Directed acyclic graphs(DAGs) are commonly used to represent causal relationships between variables. However in many cases, recovering the causal structure from observational data is NP-hard problem. \cite{ChickeringNPhard} To overcome this problem, several methods have been proposed. NOTEARS \cite{zheng2018dags} is a method that learns the structure of DAGs by minimizing the continuous approximation of the discrete constraint. This method has made it possible to learn DAGs with continuous optimization problem.

Based on the continuous acyclicty constraint, neural network based approach have been proposed. DAG-GNN \cite{yu2019daggnn} is a method that learns the structure of DAGs by using a neural network that takes the adjacency matrix of the graph as an additional input. Since the noise variable in the linear structural equation model can be interpreted as a latent variable, it uses a variational autoencoder(VAE) to learn DAGs.

\section{Directed Acyclic Graphs and Mixed Graphs}
\subsection{Directed Acyclic Graphs}

Here is the introduction of DAGs

\subsection{Mixed Graphs}

Here is the introduction of mixed graphs

\subsection{Graph Learning}

Here is the introduction of graph learning

\section{Variational Autoencoders and Normalizing Flows}

\subsection{Variational Autoencoders}

Here is the introduction of VAEs

\subsection{Normalizing Flows}

Normalizing flows are a method of transforming a simple distribution into a complex distribution. \cite{rezende15normalizingflows} Let $z$ be a random variable with a simple distribution $p(z)$, and $x$ be a random variable with a complex distribution $p(x)$. Then, we can transform $z$ into $x$ by a series of invertible transformations $f_k$ as follows:
\begin{equation}
    x = f_K \circ f_{K-1} \circ \cdots \circ f_1(z)
\end{equation}

The probability density function of $x$ can be obtained by the change of variables formula:
\begin{equation}
    p(x) = p(z) \left| \det \left( \frac{\partial f_K \circ f_{K-1} \circ \cdots \circ f_1(z)}{\partial z} \right) \right|
\end{equation}

There are several types of normalizing flows, such as Householder flow \cite{tomczak2017householder}, planar flow \cite{rezende15normalizingflows}, etc. In this paper, we use the inverse autoregressive flow (IAF) \cite{kingma2016iaf}, especially linear IAF. The linear IAF is defined as follows:
\begin{equation}
    \bf z_t = \bf \mu_t + \bf \sigma_t \odot \bf z_{t-1}.
\end{equation}
The linear IAF is the simplest case of IAF, which transforms a multivariate Gaussian with diagonal covariance to a multivariate Gaussian with full covariance. The transformation is invertible and the determinant of the Jacobian is easily computable. To use linear IAF at VAE, we need to produce an extra output $\mathbf{L(x)}$ from the encoder network. The full-covariance Gaussian distribution is obtained by the following transformation:
\begin{equation}
    \mathbf{z}_T = \mathbf{L(x)} \cdot \mathbf{z}_0.
\end{equation}
Note if we restrict the $\mathbf{L(x)}$ to be a lower triangular matrix with diagonal elements of ones, then the log-determinant of the Jacobian is zero, which is so-called volume preserving normalizing flow. \cite{kingma2016iaf}\\

With the linear IAF, the KL loss term can be written in closed form as follows:
\begin{equation}
    \begin{aligned}
        D_{KL}(q_\phi(\mathbf{z}_0 | \mathbf{x}) \Vert p(\mathbf{z}_T)) &=
        \log q_\phi(\mathbf{z}_0 | \mathbf{x}) - \log p(\mathbf{z}_T) \\
        &= -\frac{1}{2}(\mathbf{z}_0 -\mu_\phi)^T\Sigma_\phi^{-1}(\mathbf{z}_0 -\mu_\phi) + \frac{1}{2}\mathbf{z}_T^T\mathbf{z}_T \\
    \end{aligned}
\end{equation}
where $\mu_\phi$ and $\Sigma_\phi$ are the mean vector and covariance matrix from the encoder network.\\


\section{Method}

\subsection{Model}

Here is the introduction of our model

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{fig/model.png}
    \caption{Architecture of the proposed model.}
    \label{diagram}
\end{figure}

\subsection{Learning}

Here is the introduction (acyclicity constraint, etc.)

\section{Experiment}

In this section, we conduct experiments on simulated data to evaluate the performance of our proposed method. We compare our method with the DAG-GNN \cite{yu2019daggnn} with random graph datasets. We used a thresholding value of extracting graph as 0.3 which is the same value used at DAG-GNN and NOTEARS.\\ 

\paragraph*{Datasets} Random graph datasets are generated by the following procedure. First, we generate a random directed acyclic graph with Erdős–Rényi model. In Section 5.1 we conduct experiment where the noise matrix follows multivariate Gaussian with non-diagonal covariance matrix. We randomly choose edges of given proportion and make them bidirectional. Then, we generate a corresponding random covariance matrix and generate multivariate Gaussian data with the covariance matrix. We generate 5000 samples for each graph. For the size of the graph, we used 10, 20, 30, 50 nodes. For the proportion of bidirectional edges, we used 0, 0.1, 0.3, 0.5, 0.8. In Section 5.2, we considered the case where the noise matrix follows not only independent Gaussian but also independent Laplace and exponential.\\

\paragraph*{Evaluation} We evaluate the performance of our method with SHD(Structural Hamming Distance) and FDR(False Discovery Rate). SHD is the number of edge additions, deletions, and reversals needed to transform the estimated graph into the true graph. FDR is the ratio of false positives to the total number of positives. The metrics such as SHD and FDR are calculated by comparing the estimated graph with the true graph with bidirectional edges. For each combination of the number of nodes and the proportion of bidirectional edges, we generate at least 5 random graphs and calculate the average metrics.\\

\subsection{Independent Noise Case}

Here is the introduction of our experiment on independent noise case

\subsection{Dependent Noise Case}

Here is the introduction of our experiment on dependent noise case

\section{Conclusion}

Here is the conclusion

% Biblography
\bibliographystyle{ieeetr}
\bibliography{references}

\end{document}

